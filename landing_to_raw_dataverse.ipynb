{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec2eef2-71d4-41dd-b9a4-51e54cf16c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parquet to Raw: Notebook Documentation\n",
    "### Purpose\n",
    "The Parquet to Raw notebook is designed to automate the process of ingesting raw data from a landing zone into a Unity Catalog (UC) table. It reads metadata from a configuration source to dynamically process multiple tables, ensuring schema consistency and proper data handling for each table.\n",
    "\n",
    "### Functionality\n",
    "The notebook follows a structured, parallelized workflow to process data:\n",
    "\n",
    "- **Configuration Reading**: The process begins by reading a configuration source (e.g., a DataFrame or CSV file). This source acts as a manifest, containing essential metadata for each table to be processed, such as the table name, refresh mode (full_refresh, truncate, or merge), merge key, and columns to select.\n",
    "\n",
    "- **Parallel Processing**: Using a multi-threaded approach, the notebook iterates through each table defined in the configuration. This allows for concurrent processing of multiple tables, significantly reducing the overall execution time.\n",
    "\n",
    "- **Data Loading and Transformations**: For each table, the following steps are performed:\n",
    "\n",
    "1. The raw data is loaded from its Parquet file in the landing zone.\n",
    "\n",
    "2. **Column Renaming**: Special characters in column names are replaced with underscores to ensure they are compatible with Unity Catalog naming conventions.\n",
    "\n",
    "3. **Schema Enforcement**: The notebook intelligently enforces a consistent schema. If the target UC table already exists, it aligns the input DataFrame's schema to match it. If the table is new, it applies a default schema logic, converting columns ending with _date to the correct DateType.\n",
    "\n",
    "4. **Audit Fields**: Additional audit columns (aud_creationdate, aud_load_id, etc.) are added to the DataFrame for tracking.\n",
    "\n",
    "5. **Writing to Unity Catalog**: The fully prepared DataFrame is then written to the target Unity Catalog table based on the configured refresh_mode:\n",
    "\n",
    "  - `full_refresh`: Uses a `CREATE OR REPLACE` statement to completely overwrite the table, ensuring the target is a clean copy of the source data.\n",
    "  - `truncate`: Empties the existing table with a `TRUNCATE` command and then inserts the new data.\n",
    "  - `merge`: Performs a `MERGE INTO` operation, updating existing records and inserting new ones based on a specified merge_key.\n",
    "\n",
    "### Requirements\n",
    "To run this notebook successfully, the following requirements must be met:\n",
    "\n",
    "- **Databricks Cluster**: A Databricks cluster configured with access to Unity Catalog is required. The cluster must be in Single User or Shared access mode.\n",
    "\n",
    "- **Configuration Source**: A DataFrame (or equivalent data source) must exist with the necessary metadata columns, including:\n",
    "\n",
    "- `schema_name`\n",
    "- `table_name`\n",
    "- `key` (for merge operations)\n",
    "- `refresh_mode`\n",
    "- `columns` (for column filtering)\n",
    "\n",
    "**Source Data**: The raw data must be available as Parquet files in the specified landing zone path.\n",
    "\n",
    "**Permissions**: The user running the notebook must have the necessary permissions to read from the landing zone and write/modify tables in the target Unity Catalog schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2a2c21-68df-4372-b828-12b8c8e01a41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import threading\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bf6c75-dea8-445c-8809-751da009d91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: \ntable_info_file: table_info.csv\nload_id: 2025-08-12_14-16-11\nn_threads: 5\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"load_id\",\"\",\"\")\n",
    "load_id = dbutils.widgets.get(\"load_id\") or datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "dbutils.widgets.text(\"config_file\", \"\",\"\")\n",
    "config_file = dbutils.widgets.get(\"config_file\") or '/mnt/landing/it/dataverse/table_info_dataverse_beaver.csv'\n",
    "\n",
    "dbutils.widgets.text(\"table_info_file\", \"\",\"\")\n",
    "table_info_file = dbutils.widgets.get(\"table_info_file\") or 'table_info.csv'\n",
    "\n",
    "dbutils.widgets.text(\"n_threads\", \"\",\"\")\n",
    "n_threads = dbutils.widgets.get(\"n_threads\") or \"5\"\n",
    "try:\n",
    "  n_threads = int(n_threads)\n",
    "except ValueError as e:\n",
    "  n_threads = 5\n",
    "\n",
    "#### Riattivare quando porti su beaver\n",
    "# env = dbutils.secrets.get(scope=\"kv-solution-01\", key=\"environment\")\n",
    "\n",
    "# env_mapping = {\n",
    "#     \"development\": \"heiaepgb002dwe01\",\n",
    "#     \"test\": \"heiaepgb002twe01\",\n",
    "#     \"acceptance\": \"heiaepgb002awe01\",\n",
    "#     \"production\": \"heiaepgb002pwe01\"\n",
    "# }\n",
    "\n",
    "# uc_env = env_mapping[env]\n",
    "# print(uc_env)\n",
    "\n",
    "print( \"table_info_file:\", table_info_file )\n",
    "print( \"load_id:\", load_id )\n",
    "print( \"n_threads:\", n_threads )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b10376-42eb-4125-8b66-ff8927937991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_source_df(item):\n",
    "    \"\"\"\n",
    "    Loads the source DataFrame from the landing zone based on config item.\n",
    "    Args:\n",
    "        item (Row): The configuration row.\n",
    "    Returns:\n",
    "        DataFrame: The loaded DataFrame with filtered columns.\n",
    "    \"\"\"\n",
    "    path_spark = f\"dbfs:/mnt/landing/it/dataverse/\"\n",
    "    table_name = item[\"table_name\"]\n",
    "    columns_str = item[\"columns\"]\n",
    "\n",
    "    if columns_str and columns_str.strip() != \"None\":\n",
    "        cols = [c.strip() for c in columns_str.split(\",\") if c.strip()]\n",
    "    else:\n",
    "        cols = \"*\"\n",
    "\n",
    "    print(f\"Loading DataFrame for table {table_name}, selecting columns: {cols}\")\n",
    "\n",
    "    df = spark.read.format(\"parquet\").load(\n",
    "        f\"{path_spark}/{table_name}/{table_name}.parquet\"\n",
    "    )\n",
    "\n",
    "    if cols != \"*\":\n",
    "        df = df.select(*cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_rename_cols(df):\n",
    "    \"\"\"\n",
    "    Renames columns in a DataFrame by replacing special characters with underscores.\n",
    "    Args:\n",
    "        df (DataFrame): The source DataFrame.\n",
    "    \"\"\"\n",
    "    for col in df.schema:\n",
    "        df = df.withColumnRenamed(col.name, re.sub(\"[^0-9a-zA-Z]\", \"_\", col.name))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_audit_fields(df):\n",
    "    \"\"\"\n",
    "    Add technical fields to the table\n",
    "    Args:\n",
    "        df (DataFrame): The source DataFrame.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df.withColumn(\"aud_creationdate\", current_timestamp())\n",
    "        .withColumn(\"aud_modifieddate\", current_timestamp())\n",
    "        .withColumn(\"aud_load_id\", lit(load_id))\n",
    "        .withColumn(\"aud_operation\", lit(\"I\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def enforce_schema_delta(df, catalog, schema, table):\n",
    "    \"\"\"\n",
    "    Applies a schema check to an input DataFrame against an existing Delta Table.\n",
    "    If the Delta Table does not exist, it converts columns ending with '_date' to DateType.\n",
    "    Also checks for the existence of the schema.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The source DataFrame.\n",
    "        catalog (str): The Unity Catalog name.\n",
    "        schema (str): The schema name.\n",
    "        table (str): The table name.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the corrected schema.\n",
    "    \"\"\"\n",
    "    uc_table_path = f\"{catalog}.{schema}.{table}\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Correct way to check for schema existence in Unity Catalog\n",
    "    schemas = [s.name for s in spark.catalog.listDatabases(catalog)]\n",
    "\n",
    "    if schema in schemas:\n",
    "        tables = [\n",
    "            row.tableName for row in spark.catalog.listTables(f\"{catalog}.{schema}\")\n",
    "        ]\n",
    "\n",
    "        if table in tables:\n",
    "            print(f\"Delta Table '{uc_table_path}' exists. Aligning schema...\")\n",
    "            delta_df = spark.sql(f\"SELECT * FROM {uc_table_path}\")\n",
    "            delta_schema = delta_df.schema\n",
    "\n",
    "            for field in delta_schema:\n",
    "                try:\n",
    "                    col_name = field.name\n",
    "                    col_type = field.dataType\n",
    "\n",
    "                    if col_name in df.columns:\n",
    "                        if str(col_type) == \"DateType()\":\n",
    "                            df = df.withColumn(\n",
    "                                col_name, to_date(df[col_name], \"yyyy-MM-dd\")\n",
    "                            )\n",
    "                        else:\n",
    "                            df = df.withColumn(col_name, df[col_name].cast(col_type))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while casting column '{col_name}': {e}\")\n",
    "                    pass\n",
    "        else:\n",
    "            print(\"Delta Table does not exist. Applying default schema logic...\")\n",
    "            for col_name in df.columns:\n",
    "                if col_name.endswith(\"_date\"):\n",
    "                    try:\n",
    "                        df = df.withColumn(\n",
    "                            col_name, to_date(df[col_name], \"yyyy-MM-dd\")\n",
    "                        )\n",
    "                        print(f\"Column '{col_name}' converted to DateType.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not convert column '{col_name}' to DateType: {e}\")\n",
    "                        pass\n",
    "    else:\n",
    "        print(\"Schema does not exist. Applying default schema logic...\")\n",
    "        for col_name in df.columns:\n",
    "            if col_name.endswith(\"_date\"):\n",
    "                try:\n",
    "                    df = df.withColumn(col_name, to_date(df[col_name], \"yyyy-MM-dd\"))\n",
    "                    print(f\"Column '{col_name}' converted to DateType.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not convert column '{col_name}' to DateType: {e}\")\n",
    "                    pass\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_and_write_to_uc(\n",
    "    source_df, catalog, schema, table, write_mode, merge_key=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame and writes it to a Unity Catalog table using one of three modes:\n",
    "    'create' (CREATE OR REPLACE), 'truncate' (truncate and insert), or 'merge'.\n",
    "\n",
    "    Args:\n",
    "        source_df (DataFrame): The input DataFrame from the landing zone.\n",
    "        catalog (str): The Unity Catalog name.\n",
    "        schema (str): The schema name.\n",
    "        table (str): The table name.\n",
    "        write_mode (str): The write mode ('full_refresh', 'truncate', or 'merge').\n",
    "        merge_key (str, optional): The primary key for 'merge' operations. Required for merge_mode.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    uc_table_path = f\"{catalog}.{schema}.{table}\"\n",
    "    print(\n",
    "        f\"Starting data processing for table '{uc_table_path}' with mode '{write_mode}'\"\n",
    "    )\n",
    "\n",
    "    # 1. Column Renaming\n",
    "    processed_df = df_rename_cols(source_df)\n",
    "    print(\"Columns renamed successfully.\")\n",
    "\n",
    "    # 2. Schema Enforcement\n",
    "    processed_df = enforce_schema_delta(processed_df, catalog, schema, table)\n",
    "    print(\"Schema enforcement completed.\")\n",
    "\n",
    "    # Create a temporary view from the processed DataFrame\n",
    "    temp_view_name = f\"temp_view_{table}\"\n",
    "    processed_df.createOrReplaceTempView(temp_view_name)\n",
    "\n",
    "    # Check if the UC schema and table already exist\n",
    "    schemas = [s.name for s in spark.catalog.listDatabases(catalog)]\n",
    "    uc_table_exists = False\n",
    "\n",
    "    if schema not in schemas:\n",
    "        print(f\"UC schema '{schema}' not found. Creating schema...\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "        uc_table_exists = False\n",
    "    else:\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE TABLE EXTENDED {uc_table_path}\")\n",
    "            uc_table_exists = True\n",
    "        except Exception:\n",
    "            pass  # Table does not exist\n",
    "\n",
    "    # 3. Write to Unity Catalog based on the specified mode\n",
    "    if write_mode == \"full_refresh\":\n",
    "        print(f\"Writing to UC table '{uc_table_path}' using CREATE OR REPLACE...\")\n",
    "        spark.sql(\n",
    "            f\"CREATE OR REPLACE TABLE {catalog}.{schema}.{table} AS SELECT * FROM {temp_view_name}\"\n",
    "        )\n",
    "\n",
    "    elif write_mode == \"truncate\":\n",
    "        if not uc_table_exists:\n",
    "            print(\n",
    "                f\"UC table '{uc_table_path}' not found. Creating table with CREATE OR REPLACE...\"\n",
    "            )\n",
    "            spark.sql(\n",
    "                f\"CREATE OR REPLACE TABLE {catalog}.{schema}.{table} AS SELECT * FROM {temp_view_name}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Writing to UC table '{uc_table_path}' using TRUNCATE INSERT...\")\n",
    "            # Note: TRUNCATE and INSERT are not an atomic operation.\n",
    "            spark.sql(f\"TRUNCATE TABLE {catalog}.{schema}.{table}\")\n",
    "            spark.sql(\n",
    "                f\"INSERT INTO {catalog}.{schema}.{table} SELECT * FROM {temp_view_name}\"\n",
    "            )\n",
    "\n",
    "    elif write_mode == \"merge\":\n",
    "        if not merge_key:\n",
    "            raise ValueError(\"A 'merge_key' must be provided for the 'merge' mode.\")\n",
    "\n",
    "        if not uc_table_exists:\n",
    "            print(\n",
    "                f\"UC table '{uc_table_path}' not found. Creating table with CREATE OR REPLACE...\"\n",
    "            )\n",
    "            spark.sql(\n",
    "                f\"CREATE OR REPLACE TABLE {catalog}.{schema}.{table} AS SELECT * FROM {temp_view_name}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Writing to UC table '{uc_table_path}' using MERGE INTO...\")\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                MERGE INTO {catalog}.{schema}.{table} AS target\n",
    "                USING {temp_view_name} AS source\n",
    "                ON target.{merge_key} = source.{merge_key}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid write mode: '{write_mode}'. Must be 'full_refresh', 'truncate', or 'merge'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def process_table_thread(item):\n",
    "    global thread_id_count, total, ok, ko\n",
    "    global mx\n",
    "\n",
    "    id = -1\n",
    "    with mx:\n",
    "        thread_id_count = thread_id_count + 1\n",
    "        id = thread_id_count\n",
    "        print(\n",
    "            f\"[{id}] table: {item['table_name']} in schema = {item['schema_name']} is refreshing in {item['refresh_mode']} ... \"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        source_df = load_source_df(item)\n",
    "\n",
    "        process_and_write_to_uc(\n",
    "            source_df=source_df,\n",
    "            catalog=\"heiaepit002dwe01\",\n",
    "            schema=item[\"schema_name\"],\n",
    "            table=item[\"table_name\"],\n",
    "            write_mode=item[\"refresh_mode\"],\n",
    "            merge_key=item[\"key\"],\n",
    "        )\n",
    "\n",
    "        with mx:\n",
    "            print(f\"[{id}] table: {item['table_name']} ... OK\")\n",
    "            ok_tables.append(item[\"table_name\"])\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        with mx:\n",
    "            print(f\"[{id}] table: {item['table_name']} ... ERROR -> {err}\")\n",
    "            ko_tables.append({\"table\": item[\"table_name\"], \"error\": err})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef294f7-73fc-4d1f-8629-3f751a6c8850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] table: hki_product_replenishment in schema = sch_raw_dataverse_replenishment_it is refreshing in full_refresh ... \nLoading DataFrame for table hki_product_replenishment, selecting columns: *\nStarting data processing for table 'heiaepit002dwe01.sch_raw_dataverse_replenishment_it.hki_product_replenishment' with mode 'full_refresh'\n[2] table: hki_d_product_lookup in schema = sch_raw_dataverse_replenishment_it is refreshing in merge ... \nLoading DataFrame for table hki_d_product_lookup, selecting columns: *\nStarting data processing for table 'heiaepit002dwe01.sch_raw_dataverse_replenishment_it.hki_d_product_lookup' with mode 'merge'\n[3] table: hki_sales_product_custom in schema = sch_raw_dataverse_sales_it is refreshing in truncate ... \nLoading DataFrame for table hki_sales_product_custom, selecting columns: *\nStarting data processing for table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_sales_product_custom' with mode 'truncate'\n[4] table: hki_lookup_packtype_sas in schema = sch_raw_dataverse_sales_it is refreshing in full_refresh ... \nLoading DataFrame for table hki_lookup_packtype_sas, selecting columns: *\nStarting data processing for table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_lookup_packtype_sas' with mode 'full_refresh'\n[5] table: hki_lookup_packform_sas in schema = sch_raw_dataverse_sales_it is refreshing in full_refresh ... \nLoading DataFrame for table hki_lookup_packform_sas, selecting columns: *\nStarting data processing for table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_lookup_packform_sas' with mode 'full_refresh'\nColumns renamed successfully.\nColumns renamed successfully.\nColumns renamed successfully.\nColumns renamed successfully.\nColumns renamed successfully.\nSchema does not exist. Applying default schema logic...\nSchema does not exist. Applying default schema logic...\nSchema does not exist. Applying default schema logic...\nSchema does not exist. Applying default schema logic...\nSchema does not exist. Applying default schema logic...\nSchema enforcement completed.\nUC schema 'sch_raw_dataverse_replenishment_it' not found. Creating schema...\nSchema enforcement completed.\nSchema enforcement completed.\nUC table 'heiaepit002dwe01.sch_raw_dataverse_replenishment_it.hki_d_product_lookup' not found. Creating table with CREATE OR REPLACE...\nUC schema 'sch_raw_dataverse_sales_it' not found. Creating schema...\nUC schema 'sch_raw_dataverse_sales_it' not found. Creating schema...\nWriting to UC table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_lookup_packform_sas' using CREATE OR REPLACE...\nWriting to UC table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_lookup_packtype_sas' using CREATE OR REPLACE...\nSchema enforcement completed.\nUC schema 'sch_raw_dataverse_replenishment_it' not found. Creating schema...\nWriting to UC table 'heiaepit002dwe01.sch_raw_dataverse_replenishment_it.hki_product_replenishment' using CREATE OR REPLACE...\nSchema enforcement completed.\nUC schema 'sch_raw_dataverse_sales_it' not found. Creating schema...\nUC table 'heiaepit002dwe01.sch_raw_dataverse_sales_it.hki_sales_product_custom' not found. Creating table with CREATE OR REPLACE...\n[4] table: hki_lookup_packtype_sas ... OK\n[2] table: hki_d_product_lookup ... OK\n[5] table: hki_lookup_packform_sas ... OK\n[3] table: hki_sales_product_custom ... OK\n[1] table: hki_product_replenishment ... OK\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").load(f\"dbfs:{config_file}\")\n",
    "\n",
    "thread_id_count = 0\n",
    "ok_tables = list()\n",
    "ko_tables = list()\n",
    "mx = threading.Lock()\n",
    "\n",
    "pool = ThreadPool(n_threads)\n",
    "_ = pool.map(process_table_thread, df.collect())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Parquet to Raw",
   "widgets": {
    "config_file": {
     "currentValue": "",
     "nuid": "a5820fb1-afba-4521-8762-fd4b0b144146",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "config_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "config_file",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "load_id": {
     "currentValue": "",
     "nuid": "e18f31cf-6db4-4381-b0f3-fe8e77941ac2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "load_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "load_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "n_threads": {
     "currentValue": "",
     "nuid": "2181d4a1-9116-4377-888c-dafc38b73e50",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "n_threads",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "n_threads",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_info_file": {
     "currentValue": "",
     "nuid": "56732026-5a89-492f-af6d-4f9fbba56a37",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "table_info_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "table_info_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "topic": {
     "currentValue": "",
     "nuid": "2f1ce9ff-bc37-4b4f-ad0a-6b1338d9642c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "topic",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "topic",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}