{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5917ac6",
   "metadata": {},
   "source": [
    "# Landing to raw \n",
    "\n",
    "This notebook creates **raw Delta tables** in Databricks from **Parquet files** generated by Azure Data Factory (ADF).  \n",
    "It receives a **table list configuration** from the previous notebook and processes each table by:\n",
    "- Loading the data from the landing zone.\n",
    "- Renaming columns to meet Delta/SQL naming rules.\n",
    "- Adding audit metadata fields.\n",
    "- Enforcing schema consistency with existing Delta tables.\n",
    "- Creating or overwriting the final raw table in the target schema.\n",
    "\n",
    "Tables are processed **in parallel** to improve performance, and the notebook returns a JSON summary of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee023574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02111231",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "| Name              | Type     | Default | Description |\n",
    "|-------------------|----------|---------|-------------|\n",
    "| `load_id`         | string   | current timestamp | Load identifier for tracking. |\n",
    "| `table_info_file` | string   | `table_info.csv` | CSV file with table names and optional column selection. |\n",
    "| `n_threads`       | integer  | 5       | Number of threads for parallel table processing. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"load_id\",\"\",\"\")\n",
    "load_id = dbutils.widgets.get(\"load_id\") or datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "dbutils.widgets.text(\"table_info_file\", \"\",\"\")\n",
    "table_info_file = dbutils.widgets.get(\"table_info_file\") or 'table_info.csv'\n",
    "\n",
    "dbutils.widgets.text(\"n_threads\", \"\",\"\")\n",
    "n_threads = dbutils.widgets.get(\"n_threads\") or \"5\"\n",
    "try:\n",
    "  n_threads = int(n_threads)\n",
    "except ValueError as e:\n",
    "  n_threads = 5\n",
    "\n",
    "\n",
    "print( \"table_info_file:\", table_info_file )\n",
    "print( \"load_id:\", load_id )\n",
    "print( \"n_threads:\", n_threads )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15de3d8",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### `df_rename_cols(df)`\n",
    "Renames all columns by replacing non-alphanumeric characters with `_`.\n",
    "\n",
    "### `add_audit_fields(df)`\n",
    "Adds:\n",
    "- `aud_creationdate`\n",
    "- `aud_modifieddate`\n",
    "- `aud_load_id`\n",
    "- `aud_operation` (set to `'I'`)\n",
    "\n",
    "### `enforce_schema_delta(df, delta_table_path)`\n",
    "If the Delta table exists:\n",
    "- Matches column types to the existing schema.\n",
    "- Parses date columns using the configured date format.\n",
    "\n",
    "### `create_db_table(df, table_name, delta_path, delta_path_spark)`\n",
    "Writes DataFrame as Delta, creates/overwrites the schema and table in the Databricks metastore, and runs `VACUUM`.\n",
    "\n",
    "### `process_table(table_name, config, encoding='UTF-8')`\n",
    "Full processing sequence:\n",
    "1. Read Parquet file.\n",
    "2. Apply column renaming.\n",
    "3. Add audit fields.\n",
    "4. Enforce schema.\n",
    "5. Create/overwrite Delta table.\n",
    "\n",
    "### `process_table_thread(table_name, target_schema=None)`\n",
    "Thread-safe wrapper for `process_table` with status logging and error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_schema_delta(df, delta_table_path):\n",
    "    if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "        df_delta = spark.read.format('delta').load(delta_table_path)\n",
    "        delta_schema = df_delta.schema\n",
    "        for field in delta_schema:\n",
    "            try:\n",
    "                name = field.name\n",
    "                if name == re.sub('[^0-9a-zA-Z]', '_', table_metadata[\"date_col\"]):\n",
    "                    dateformat = 'yyyy-MM-dd'\n",
    "                    if table_metadata['date_format'] != '':\n",
    "                        dateformat = str(table_metadata['date_format'])\n",
    "                    df = df.withColumn(partition_col, to_date(df[partition_col], dateformat))\n",
    "                else:\n",
    "                    df = df.withColumn(name, df[name].cast(field.dataType))\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"not a delta table\")\n",
    "    return df\n",
    "\n",
    "def df_rename_cols(df):\n",
    "    for col in df.schema:\n",
    "        df = df.withColumnRenamed(col.name, re.sub('[^0-9a-zA-Z]', '_', col.name))\n",
    "    return df\n",
    "\n",
    "def add_audit_fields(df):\n",
    "    df = df \\\n",
    "        .withColumn('aud_creationdate', current_timestamp()) \\\n",
    "        .withColumn('aud_modifieddate', current_timestamp()) \\\n",
    "        .withColumn('aud_load_id', lit(load_id)) \\\n",
    "        .withColumn('aud_operation', lit('I'))\n",
    "    return df\n",
    "\n",
    "def create_db_table(df, table_name, delta_path, delta_path_spark):\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(delta_path_spark)\n",
    "    except:\n",
    "        pass\n",
    "    df.write.format('delta').mode('overwrite').option('overwriteSchema', True).save(delta_path_spark)\n",
    "    schema_name = '<your_schema>'\n",
    "    print(f\"creating databricks table: {schema_name}.{table_name}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {schema_name}.{table_name}\")\n",
    "    spark.sql(f'CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} USING DELTA LOCATION \"{delta_path_spark}\"')\n",
    "    spark.sql(f'VACUUM {schema_name}.{table_name}')\n",
    "\n",
    "def process_table(table_name, config, encoding='UTF-8'):\n",
    "    global topic_path_spark\n",
    "    print(f\"[DEBUG] process_table START for table_name={table_name}\")\n",
    "    delta_path = f\"/dbfs/mnt/dls/path/to/raw/{table_name}\"\n",
    "    delta_path_spark = f\"dbfs:/mnt/dls/path/to/raw/{table_name}\"\n",
    "\n",
    "    print(f\"[DEBUG] delta_path: {delta_path}\")\n",
    "    print(f\"[DEBUG] delta_path_spark: {delta_path_spark}\")\n",
    "\n",
    "    try:\n",
    "        row = config.filter(col(\"table_name\") == table_name).limit(1).collect()\n",
    "        print(f\"[DEBUG] config row: {row}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at config row extraction: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        if row and \"columns\" in row[0].asDict():\n",
    "            cols_str = row[0][\"columns\"]\n",
    "            if cols_str is not None and cols_str.strip() != \"\":\n",
    "                cols = [c.strip() for c in cols_str.split(\";\") if c.strip() != \"\"]\n",
    "            else:\n",
    "                cols = \"*\"\n",
    "        else:\n",
    "            cols = \"*\"\n",
    "        print(f\"[DEBUG] columns to select: {cols}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at columns extraction: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        if cols != \"*\":\n",
    "            df = spark.read.option(\"infer_schema\", False).parquet(f\"{topic_path_spark}/{table_name}\").select(*cols)\n",
    "        else:\n",
    "            df = spark.read.option(\"infer_schema\", False).parquet(f\"{topic_path_spark}/{table_name}\")\n",
    "        print(f\"[DEBUG] DataFrame loaded for table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at DataFrame loading: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        df = df_rename_cols(df)\n",
    "        print(f\"[DEBUG] Columns renamed for table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at df_rename_cols: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        df = add_audit_fields(df)\n",
    "        print(f\"[DEBUG] Audit fields added for table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at add_audit_fields: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        df = enforce_schema_delta(df, delta_path_spark)\n",
    "        print(f\"[DEBUG] enforce_schema_delta done for table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at enforce_schema_delta: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        create_db_table(df, table_name, delta_path, delta_path_spark, )\n",
    "        print(f\"[DEBUG] create_db_table done for table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed at create_db_table: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_table_thread( table_name, target_schema=None ):\n",
    "  global thread_id_count, total, ok, ko\n",
    "  global mx\n",
    "\n",
    "  id = -1\n",
    "  with mx:\n",
    "    thread_id_count = thread_id_count + 1\n",
    "    id = thread_id_count\n",
    "    print(f\"[{id}] table: {table_name} ... \")\n",
    "  \n",
    "  try:\n",
    "    process_table( table_name, df_table_info_ext)\n",
    "    with mx:\n",
    "      print(f\"[{id}] table: {table_name} ... OK\")\n",
    "      ok_tables.append(table_name)\n",
    "  except Exception as e:\n",
    "    err = str(e)\n",
    "    # err_brief = str(err).split('\\n')[0]\n",
    "    with mx:\n",
    "      print( f\"[{id}] table: {table_name} ... ERROR -> {err}\" )\n",
    "      ko_tables.append({ 'table' : table_name, 'error' : err })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7624605",
   "metadata": {},
   "source": [
    "## Execution Flow\n",
    "\n",
    "1. **Load configuration**  \n",
    "   - Read `table_info.csv` from the landing zone.  \n",
    "   - Join with extra CSV for column selection (optional).\n",
    "\n",
    "2. **Start threaded processing**  \n",
    "   - Initialize thread pool with `n_threads`.  \n",
    "   - Process each table with `process_table_thread`.\n",
    "\n",
    "3. **Collect results**  \n",
    "   - Track successful and failed tables.  \n",
    "   - Print JSON summary.\n",
    "\n",
    "4. **Exit or raise error**  \n",
    "   - If all tables succeed → return JSON.  \n",
    "   - If any fail → raise exception with details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_path = f\"/dbfs/mnt/landing/path/to\"\n",
    "topic_path_spark = f\"dbfs:/mnt/landing/path/to\"\n",
    "\n",
    "table_info_path = f\"{topic_path}/{table_info_file}\"\n",
    "table_info_path_spark = f\"{topic_path_spark}/{table_info_file}\"\n",
    "\n",
    "print( f\"table info file path: '{table_info_path}'\" )\n",
    "\n",
    "\n",
    "df_table_info = spark.read.option(\"header\", True).option(\"infer_schema\", False).csv(table_info_path_spark).distinct()\n",
    "\n",
    "df_from_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").load(\"file:/path/to/dataverse_table_info.csv\")\n",
    "df_from_csv = df_from_csv.select(\"table_name\", \"columns\")\n",
    "\n",
    "df_table_info_ext = df_table_info.join(df_from_csv, on=\"table_name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b638465",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_count = 0\n",
    "ok_tables = list()\n",
    "ko_tables = list()\n",
    "mx = threading.Lock()\n",
    "\n",
    "pool = ThreadPool(n_threads)\n",
    "_ = pool.map( lambda row : process_table_thread( row['table_name'], (row['custom_target_schema']) if 'custom_target_schema' in df_table_info_ext.columns else None ) , df_table_info_ext.collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_return = {\n",
    "  'status' : ( 'success' if len(ko_tables) == 0 else 'error' ),\n",
    "  'ok_count' : len(ok_tables),\n",
    "  'ko_count' : len(ko_tables),\n",
    "  'ok_tables' : ok_tables,\n",
    "  'ko_tables' : ko_tables,\n",
    "  'status_detail' : ( '' if len(ko_tables) == 0 else 'failed tables during the loading process!' ),\n",
    "}\n",
    "\n",
    "print( json.dumps(to_return, indent=4) )\n",
    "\n",
    "if to_return['status'] == 'success':\n",
    "  dbutils.notebook.exit( to_return )\n",
    "else:\n",
    "  raise Exception( to_return )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
