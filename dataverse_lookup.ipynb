{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab4e7",
   "metadata": {},
   "source": [
    "# Dataverse Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import openpyxl\n",
    "import dbutils\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cfea1",
   "metadata": {},
   "source": [
    "## üìÑ Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description / Usage |\n",
    "|-----------|------|---------|---------------------|\n",
    "| `country` | string | `\"it\"` | Country code used to filter or customize execution. Currently only printed, but can be used for logic routing. |\n",
    "| `topic` | string | `\"dataverse\"` | Data domain or topic (e.g., `\"dataverse\"`). Used to filter tables based on the `topic` column in the input CSV file. |\n",
    "| `target_table_info_name` | string | `\"table_info_dataverse.csv\"` | Name of the CSV file containing target table information. Not directly loaded in this script (uses `dataverse_table_info.csv` hardcoded), but can be used in extensions. |\n",
    "| `max_age` | int (seconds) | `86400` (24h) | Maximum allowed age (in seconds) since last modification of a table before it‚Äôs considered for refresh. If the table is ‚Äúyounger‚Äù than this limit, it will be refreshed. |\n",
    "| `force_refresh_all` | boolean | `false` | If `true`, forces refresh of **all** tables regardless of `max_age` or row count checks. |\n",
    "| `select_topics` | string (semicolon-separated list) | `None` | List of topics to include. If set, only tables with a `topic` value present in this list are processed. A trailing `;` is removed if present. |\n",
    "| `force_fail` | boolean | `false` | If `true`, the notebook will raise an exception in case of an error when fetching Dataverse table metadata; otherwise, it will log the error and continue. |\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Operational Notes\n",
    "- Parameters are created using `dbutils.widgets` so they can be set via the Databricks UI or orchestrated from Azure Data Factory.\n",
    "- **Azure Key Vault secrets** (`dataprocessing-tenant-id`, `dataprocessing-app-id`, `dataprocessing-app-secret`, `dataverse-uri`, etc.) are **used inside the notebooks** to authenticate with Dataverse. They are **not part of the ADF pipeline definition**.\n",
    "- `select_topics` is normalized into a Python `set` for fast lookups.\n",
    "- `max_age` is in **seconds** (`86400` = 24 hours).\n",
    "- `force_refresh_all` and `force_fail` are dropdown boolean widgets in Databricks (`true` / `false`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee90e2",
   "metadata": {},
   "source": [
    "## üìö Function Reference\n",
    "\n",
    "This section describes the core utility functions used for:\n",
    "- Tracking Dataverse table metadata (last update date & row count).\n",
    "- Authenticating to the Dataverse API.\n",
    "- Retrieving table status and counts.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_stat_format(table_topic, table_name_plural, last_modified_dt, last_modified_count)`\n",
    "\n",
    "**Purpose:**  \n",
    "Returns a Python dictionary matching the `stats_tab_schema` format for easier DataFrame creation.\n",
    "\n",
    "**Parameters:**\n",
    "- `table_topic` *(str)* ‚Äì Logical topic/group the table belongs to.\n",
    "- `table_name_plural` *(str)* ‚Äì Dataverse plural table name.\n",
    "- `last_modified_dt` *(datetime)* ‚Äì Last modification timestamp.\n",
    "- `last_modified_count` *(int)* ‚Äì Row count at last update.\n",
    "\n",
    "**Returns:**  \n",
    "A dictionary with keys:  \n",
    "`table_topic`, `table_name_plural`, `last_modified_dt`, `last_modified_count`.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_table_stats(table_name_plural)`\n",
    "\n",
    "**Purpose:**  \n",
    "Checks whether statistics for a given Dataverse table already exist in `<your_schema>.dataverse_update_lookup`.\n",
    "\n",
    "**Parameters:**\n",
    "- `table_name_plural` *(str)* ‚Äì Dataverse plural table name.\n",
    "\n",
    "**Returns:**  \n",
    "Tuple:\n",
    "1. `found?` *(bool)* ‚Äì Whether an entry exists.\n",
    "2. `count` *(int or None)* ‚Äì Stored row count.\n",
    "3. `last_dt` *(datetime or None)* ‚Äì Stored last modification date.\n",
    "\n",
    "---\n",
    "\n",
    "### `update_table_stats(table_topic, table_name_plural, last_modified_dt, last_modified_count)`\n",
    "\n",
    "**Purpose:**  \n",
    "Updates or inserts statistics for a Dataverse table in `<your_schema>.dataverse_update_lookup`.\n",
    "\n",
    "**Parameters:**\n",
    "- `table_topic` *(str)*\n",
    "- `table_name_plural` *(str)*\n",
    "- `last_modified_dt` *(datetime)*\n",
    "- `last_modified_count` *(int)*\n",
    "\n",
    "**Notes:**\n",
    "- If the record does not exist, it is **inserted**.\n",
    "- If the record exists, the `last_modified_dt` and `last_modified_count` fields are **updated**.\n",
    "- ‚ö†Ô∏è If two processes run the Dataverse lookup in parallel, they may both update the same table. This is not critical ‚Äî at worst, the table will be refreshed twice.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_access_token(tenant_id, client_id, client_secret, dataverse_uri)`\n",
    "\n",
    "**Purpose:**  \n",
    "Requests an OAuth 2.0 client credentials access token for the Dataverse API.\n",
    "\n",
    "**Parameters:**\n",
    "- `tenant_id` *(str)* ‚Äì Azure AD tenant ID.\n",
    "- `client_id` *(str)* ‚Äì Application client ID.\n",
    "- `client_secret` *(str)* ‚Äì Application secret.\n",
    "- `dataverse_uri` *(str)* ‚Äì Dataverse instance URI (without `/api/data/v9.2/`).\n",
    "\n",
    "**Returns:**  \n",
    "Access token *(str)* to be used in HTTP `Authorization` headers.\n",
    "\n",
    "**Raises:**  \n",
    "Exception if the token request fails or the response does not contain `access_token`.\n",
    "\n",
    "---\n",
    "\n",
    "### `dataverse_count(table_name_plural, access_token)`\n",
    "\n",
    "**Purpose:**  \n",
    "Counts the number of rows in a Dataverse table by paging through results.\n",
    "\n",
    "**Parameters:**\n",
    "- `table_name_plural` *(str)* ‚Äì Dataverse plural table name.\n",
    "- `access_token` *(str)* ‚Äì OAuth 2.0 access token.\n",
    "\n",
    "**Returns:**  \n",
    "Row count *(int)* for the specified table.\n",
    "\n",
    "**Notes:**\n",
    "- Uses `$select=modifiedon` and `$count=true` to minimize payload.\n",
    "- Follows `@odata.nextLink` for pagination.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_status_of_dataverse_table(table_name_plural, endpoint, access_token)`\n",
    "\n",
    "**Purpose:**  \n",
    "Retrieves the latest modification date and row count for a Dataverse table.\n",
    "\n",
    "**Parameters:**\n",
    "- `table_name_plural` *(str)* ‚Äì Dataverse plural table name.\n",
    "- `endpoint` *(str)* ‚Äì Dataverse API endpoint (e.g., `https://<org>.crm.dynamics.com/api/data/v9.2`).\n",
    "- `access_token` *(str)* ‚Äì OAuth 2.0 access token.\n",
    "\n",
    "**Returns:**  \n",
    "Tuple:\n",
    "1. `last_modified_date` *(datetime or None)* ‚Äì Latest modification timestamp.\n",
    "2. `row_count` *(int or None)* ‚Äì Total number of rows.\n",
    "3. `response` *(requests.Response)* ‚Äì Original HTTP response object.\n",
    "\n",
    "**Behavior:**\n",
    "- Orders results by `modifiedon` descending and retrieves the most recent record.\n",
    "- If no records exist, returns `datetime(1970, 1, 1, 0, 0, 1)` and count `0`.\n",
    "- Calls `dataverse_count()` to get the full row count.\n",
    "\n",
    "**Error Handling:**\n",
    "- If the HTTP status code is not `200` or `202`, prints a warning and returns `(None, None, response)`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Key Notes for GitHub Usage\n",
    "- Replace `<your_schema>` with a schema name matching your Dataverse topic (e.g., `sch_statistics`).\n",
    "- These functions are designed to be **idempotent** ‚Äî running them multiple times will not cause data corruption.\n",
    "- The statistics tracking table is critical for **incremental refresh logic** in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_format(table_topic, table_name_plural, last_modified_dt, last_modified_count):\n",
    "    return {\n",
    "        \"table_topic\": table_topic,\n",
    "        \"table_name_plural\": table_name_plural,\n",
    "        \"last_modified_dt\": last_modified_dt,\n",
    "        \"last_modified_count\": last_modified_count,\n",
    "    }\n",
    "\n",
    "def get_table_stats(table_name_plural):\n",
    "    \"\"\"\n",
    "    Returns: (found?, count, last_dt)\n",
    "    \"\"\"\n",
    "    global df_stats\n",
    "    data = df_stats[df_stats['table_name_plural'] == table_name_plural].collect()\n",
    "    if len(data) == 0:\n",
    "        return False, None, None\n",
    "    return True, data[0]['last_modified_count'], data[0]['last_modified_dt']\n",
    "\n",
    "def update_table_stats(table_topic, table_name_plural, last_modified_dt, last_modified_count):\n",
    "    global df_stats, stats_tab_schema\n",
    "\n",
    "    data = df_stats[df_stats['table_name_plural'] == table_name_plural].collect()\n",
    "    if len(data) == 0:\n",
    "        spark.createDataFrame(\n",
    "            data=[get_stat_format(table_topic, table_name_plural, last_modified_dt, last_modified_count)],\n",
    "            schema=stats_tab_schema\n",
    "        ).write.insertInto(\"<your_schema>.dataverse_update_lookup\")\n",
    "    else:\n",
    "        _ = spark.sql(f\"\"\"\n",
    "        UPDATE <your_schema>.dataverse_update_lookup\n",
    "        SET \n",
    "          last_modified_dt = to_timestamp('{last_modified_dt.strftime('%Y%m%d %H%M%S')}', 'yyyyMMdd HHmmss'),\n",
    "          last_modified_count = {last_modified_count}\n",
    "        WHERE  \n",
    "          table_name_plural = '{table_name_plural}'\n",
    "        \"\"\")\n",
    "\n",
    "def get_access_token(tenant_id, client_id, client_secret, dataverse_uri):\n",
    "    token_endpoint_url = (\n",
    "        f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "        \"scope\": f\"{dataverse_uri}/.default\", \n",
    "    }\n",
    "    res = requests.post(token_endpoint_url, data=payload)\n",
    "    if res.status_code != 200:\n",
    "        raise Exception(f\"ERROR while trying getting token from dataverse service! Reason:\\n{res.content}\")\n",
    "    access_token = ''\n",
    "    try:\n",
    "        access_token = res.json()[\"access_token\"]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"ERROR while trying getting token from dataverse service! Reason:\\n{e}\")\n",
    "    return access_token\n",
    "\n",
    "def dataverse_count(table_name_plural, access_token):\n",
    "  arg = f\"{table_name_plural}?$select=modifiedon&$count=true\"\n",
    "  headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "  }\n",
    "\n",
    "  count_val = 0\n",
    "  url = f\"{endpoint}/{arg}\".replace(' ', '%20')\n",
    "  while True:\n",
    "    res = requests.get(url=url, headers=headers)\n",
    "    count_val += len(res.json()['value'])\n",
    "\n",
    "    if '@odata.nextLink' in res.json().keys():\n",
    "      arg = res.json()['@odata.nextLink']\n",
    "      url = f\"{arg}\".replace(' ', '%20')\n",
    "    else:\n",
    "      break\n",
    "  \n",
    "  return count_val\n",
    "\n",
    "def get_status_of_dataverse_table(table_name_plural, endpoint, access_token):\n",
    "    url = f\"{endpoint}/{table_name_plural}?$count=true&$orderby=modifiedon desc&$top=1&$select=modifiedon\".replace(' ', '%20')\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    res = requests.get(\n",
    "        url=url, \n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "    if res.status_code not in (200, 202, ):\n",
    "        print(\"WARNING:\", res.status_code, \"with content\", res.content)\n",
    "        return None, None, res\n",
    "\n",
    "    res_json = res.json()['value']\n",
    "    last_modified_date = datetime(1970, 1, 1, 0, 0, 1)\n",
    "    if len(res_json) > 0:\n",
    "        return datetime.fromisoformat(res_json[0]['modifiedon'].replace('Z', '')), dataverse_count(table_name_plural, access_token), res\n",
    "    else:\n",
    "        return last_modified_date, 0, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb27523",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdbutils\u001b[49m.widgets.text( \u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m ) \n\u001b[32m      2\u001b[39m country = dbutils.widgets.get( \u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m dbutils.widgets.text( \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m ) \n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text( \"country\", \"\",\"\" ) \n",
    "country = dbutils.widgets.get( \"country\" ) or \"it\"\n",
    "\n",
    "dbutils.widgets.text( \"topic\", \"\",\"\" ) \n",
    "topic = dbutils.widgets.get( \"topic\" ) or \"dataverse\"\n",
    "\n",
    "dbutils.widgets.text( \"target_table_info_name\", \"\",\"\" ) \n",
    "target_table_info_name = dbutils.widgets.get( \"target_table_info_name\" ) or \"table_info_dataverse.csv\"\n",
    "\n",
    "dbutils.widgets.text( \"max_age\", \"\",\"\" ) # in seconds\n",
    "max_age = int(dbutils.widgets.get( \"max_age\" ) or \"86400\") # DEFAULT: 24 hours\n",
    "\n",
    "dbutils.widgets.dropdown('force_refresh_all', 'false', ['true', 'false'])\n",
    "force_refresh_all = (dbutils.widgets.get( \"force_refresh_all\" ) or \"false\") == 'true'\n",
    "\n",
    "dbutils.widgets.text( \"select_topics\", \"\",\"\" )\n",
    "select_topics = (dbutils.widgets.get( \"select_topics\" ) or '').strip()\n",
    "select_topics = select_topics[:-1] if select_topics.endswith(';') else select_topics\n",
    "if select_topics in ('', ';'):\n",
    "    select_topics = None\n",
    "else:\n",
    "    select_topics = set(select_topics.split(';'))\n",
    "\n",
    "dbutils.widgets.dropdown('force_fail', 'false', ['true', 'false'])\n",
    "force_fail = (dbutils.widgets.get( \"force_fail\" ) or \"false\") == 'true'\n",
    "\n",
    "print(\"country:\", country)\n",
    "print(\"topic:\", topic)\n",
    "print(\"max_age:\", max_age)\n",
    "print(\"force_refresh_all:\", force_refresh_all)\n",
    "print(\"force_fail:\", force_fail)\n",
    "print(\"select_topics:\", select_topics)\n",
    "print(\"target_table_info_name:\", target_table_info_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dbutils.secrets.get(scope=\"<your-key-vault-solution>\", key=\"<your-environment-key>\")\n",
    "\n",
    "env_mapping = {\n",
    "    \"development\": \"<your-dev-catalog>\",\n",
    "    \"test\": \"<<your-dev-catalog>>\",\n",
    "    \"acceptance\": \"<your-acceptance-catalog>\",\n",
    "    \"production\": \"<your-production-catalog>\"\n",
    "}\n",
    "\n",
    "uc_env = env_mapping[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce522d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").load(\"/path/to/config.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66812477",
   "metadata": {},
   "source": [
    "## üìä Dataverse Table Statistics Management\n",
    "\n",
    "This section of the notebook manages a **Delta table** used to track the last modification date and row count for Dataverse tables.  \n",
    "It allows the pipeline to decide whether a table needs refreshing based on changes in Dataverse.\n",
    "\n",
    "`<your_schema>` should be replaced with the schema corresponding to your data topic (e.g., sch_statistics, sch_sales, etc.).\n",
    "\n",
    "### The statistics table:\n",
    "`dataverse_update_lookup` is a Delta table used to store:\n",
    "- **table_topic** ‚Äì The logical topic/group the table belongs to.\n",
    "- **table_name_plural** ‚Äì The Dataverse API plural table name.\n",
    "- **last_modified_count** ‚Äì Total number of rows at last update.\n",
    "- **last_modified_dt** ‚Äì Timestamp of the most recent modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a250cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema if it does not exist\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS <your_schema>;\")\n",
    "\n",
    "# Create the Delta table to store update statistics if it does not exist\n",
    "_ = spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS <your_schema>.dataverse_update_lookup\n",
    "(\n",
    "  table_topic STRING,\n",
    "  table_name_plural STRING, \n",
    "  last_modified_count INT,\n",
    "  last_modified_dt TIMESTAMP\n",
    ")\n",
    "USING DELTA LOCATION 'dbfs:/mnt/dls/path/to/dataverse_update_lookup';\n",
    "\"\"\")\n",
    "\n",
    "# Load the current statistics table into a DataFrame\n",
    "df_stats = spark.sql(f\"SELECT * FROM <your_schema>.dataverse_update_lookup\")\n",
    "\n",
    "# Define the structure of the DataFrame that will be written into the Delta table.\n",
    "stats_tab_schema = StructType([\n",
    "  StructField(\"table_topic\", StringType(), True),\n",
    "  StructField(\"table_name_plural\", StringType(), True),\n",
    "  StructField(\"last_modified_count\", IntegerType(), True),\n",
    "  StructField(\"last_modified_dt\", TimestampType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e13a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dataverse\n",
    "\n",
    "dataprocessing_tenant_id = dbutils.secrets.get(scope=\"<your-key-vault-solution>\",key=\"<your-dataprocessing-tenant-id>\")\n",
    "dataprocessing_client_id = dbutils.secrets.get(scope=\"<your-key-vault-solution>\",key=\"<your-dataprocessing-client-id>\")\n",
    "dataprocessing_client_secret = dbutils.secrets.get(scope=\"<your-key-vault-solution>\",key=\"<your-dataprocessing-client-secret>\")\n",
    "dataverse_uri = dbutils.secrets.get(scope=\"<your-key-vault-solution>\",key=\"<your-dataverse-uri>\")\n",
    "\n",
    "access_token = get_access_token(\n",
    "    dataprocessing_tenant_id, \n",
    "    dataprocessing_client_id, \n",
    "    dataprocessing_client_secret, \n",
    "    dataverse_uri\n",
    ")\n",
    "\n",
    "endpoint = f\"{dataverse_uri}/api/data/v9.2\"\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fbbc5",
   "metadata": {},
   "source": [
    "## üöÄ Execution Logic ‚Äì Selecting Tables to Refresh\n",
    "\n",
    "This script is the **execution entry point** that:\n",
    "1. Iterates over the list of Dataverse tables (`df`).\n",
    "2. Checks their **last modified date** and **row count** against stored metadata.\n",
    "3. Decides whether each table should be refreshed.\n",
    "4. Produces:\n",
    "   - `tables_to_refresh_list` ‚Äì Python list with refresh metadata.\n",
    "   - `table_info_output_csv` ‚Äì CSV configuration for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now is the reference timestamp for freshness checks.\n",
    "# tables_to_refresh_list will store metadata for tables needing refresh.\n",
    "# table_info_output_csv starts with the CSV header for pipeline configuration.\n",
    "now = datetime.now()\n",
    "tables_to_refresh_list = list()\n",
    "table_info_output_csv = \"schema,table_name,primary_key,mode,where_condition,date_col,date_format,custom_target_schema\\n\"\n",
    "\n",
    "# df contains the tables to check, along with schema/topic info\n",
    "for item in df.collect():\n",
    "    # If select_topics is set, skip tables outside the selected topics.\n",
    "    if select_topics is not None and item['topic'] not in select_topics:\n",
    "        print(f\"Skipping table '{item['table_name']}' (not in topics list)\")\n",
    "        continue\n",
    "\n",
    "    # From Dataverse: Get latest modified date & total row count.\n",
    "    # From Metadata Store: Check if the table is already tracked (ds_found) and retrieve stored stats.\n",
    "    last_modified_date, last_modified_count, res = get_status_of_dataverse_table(item['table_name_plural'], endpoint, access_token)\n",
    "    ds_found, ds_count, ds_last_dt = get_table_stats(item['table_name_plural'])\n",
    "\n",
    "    if last_modified_date is None:\n",
    "        msg = f\"ERROR while checking last update for table '{item['table_name']}'\"\n",
    "        if force_fail:\n",
    "            raise Exception(msg)\n",
    "        else:\n",
    "            print(msg)\n",
    "            continue\n",
    "\n",
    "# A table is eligible for refresh if:\n",
    "    # It has never been tracked OR its row count has changed.\n",
    "    # OR It has been modified within the allowed age window (max_age seconds).\n",
    "    # OR force_refresh_all is enabled.\n",
    "\n",
    "    # CHECK 1: rows number\n",
    "    check_rows = (not ds_found) or (ds_found and (last_modified_count != ds_count))\n",
    "\n",
    "    # CHECK 2 last modified \n",
    "    age = now - last_modified_date\n",
    "    check_last_modified_dt = age.total_seconds() <= max_age\n",
    "\n",
    "    if check_rows or check_last_modified_dt or force_refresh_all:\n",
    "        if force_refresh_all:\n",
    "            print(f\"FORCE REFRESH table '{item['table_name']}' to list of tables to refresh with last update: {last_modified_date} ({age})\")\n",
    "        else:\n",
    "            print(f\"Adding table '{item['table_name']}' to list of tables to refresh with last update: {last_modified_date} ({age})\")\n",
    "\n",
    "        # Append a dictionary with refresh metadata to tables_to_refresh_list.\n",
    "        # Add a line to the CSV config (overwrite mode by default).\n",
    "        # Update the metadata store with the latest date & count.\n",
    "        tables_to_refresh_list.append({\n",
    "            'table_name' : item['table_name'],\n",
    "            'table_name_plural' : item['table_name_plural'],\n",
    "            'schema' : item['schema_name'],\n",
    "            'current_date' : str(now),\n",
    "            'last_modified_date' : str(last_modified_date),\n",
    "            'age' : age.total_seconds()\n",
    "        })\n",
    "\n",
    "        table_info_output_csv += f\",{item['table_name']},,overwrite,,,,{item['schema_name']}\\n\"\n",
    "\n",
    "        update_table_stats(item['topic'], item['table_name_plural'], last_modified_date, last_modified_count)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Skipping table '{item['table_name']}' with last update: {last_modified_date} ({age})\")\n",
    "\n",
    "# Create the target directory in Databricks File System.\n",
    "# Remove any existing CSV file.\n",
    "# Save the generated table_info_output_csv for the pipeline.\n",
    "table_info_path = f\"dbfs:/mnt/landing/path/to\"\n",
    "table_info_file_path = f\"dbfs:/mnt/landing/path/to/{target_table_info_name}\"\n",
    "print(table_info_file_path)\n",
    "print(table_info_path)\n",
    "dbutils.fs.mkdirs(table_info_path)\n",
    "dbutils.fs.rm(table_info_file_path)\n",
    "dbutils.fs.put(table_info_file_path, contents=table_info_output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit and output for Datafactory\n",
    "dbutils.notebook.exit({\n",
    "    \"tables\" : tables_to_refresh_list,\n",
    "    \"table_info_name\" : target_table_info_name\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
